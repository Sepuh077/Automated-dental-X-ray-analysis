{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, backbone=\"resnet18\"):\n",
    "        '''\n",
    "        Creates a siamese network with a network from torchvision.models as backbone.\n",
    "            Parameters:\n",
    "                    backbone (str): Options of the backbone networks can be found at https://pytorch.org/vision/stable/models.html\n",
    "        '''\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if backbone not in models.__dict__:\n",
    "            raise Exception(\"No model named {} exists in torchvision.models.\".format(backbone))\n",
    "\n",
    "        # Create a backbone network from the pretrained models provided in torchvision.models \n",
    "        self.backbone = models.__dict__[backbone](pretrained=True, progress=True)\n",
    "\n",
    "        # Get the number of features that are outputted by the last layer of backbone network.\n",
    "        out_features = list(self.backbone.modules())[-1].out_features\n",
    "\n",
    "        # Create an MLP (multi-layer perceptron) as the classification head. \n",
    "        # Classifies if provided combined feature vector of the 2 images represent same player or different.\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(out_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.5),\n",
    "\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        '''\n",
    "        Returns the similarity value between two images.\n",
    "            Parameters:\n",
    "                    img1 (torch.Tensor): shape=[b, 3, 224, 224]\n",
    "                    img2 (torch.Tensor): shape=[b, 3, 224, 224]\n",
    "            where b = batch size\n",
    "            Returns:\n",
    "                    output (torch.Tensor): shape=[b, 1], Similarity of each pair of images\n",
    "        '''\n",
    "\n",
    "        # Pass the both images through the backbone network to get their seperate feature vectors\n",
    "        feat1 = self.backbone(img1)\n",
    "        feat2 = self.backbone(img2)\n",
    "        \n",
    "        # Multiply (element-wise) the feature vectors of the two images together, \n",
    "        # to generate a combined feature vector representing the similarity between the two.\n",
    "        combined_features = feat1 * feat2\n",
    "\n",
    "        # Pass the combined feature vector through classification head to get similarity value in the range of 0 to 1.\n",
    "        output = self.cls_head(combined_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 50)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_image(path):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.resize(\n",
    "        cv2.cvtColor(img, cv2.COLOR_BGR2GRAY),\n",
    "        (100, 100)\n",
    "    )\n",
    "    return torch.tensor(img[None, None]).float().to('cuda')\n",
    "\n",
    "healthy = [get_image('./data_siamese/0/' + img) for img in os.listdir('./data_siamese/0/')]\n",
    "unhealthy = [get_image('./data_siamese/1/' + img) for img in os.listdir('./data_siamese/1/')]\n",
    "\n",
    "shfl0 = list(range(len(healthy)))\n",
    "random.shuffle(shfl0)\n",
    " \n",
    "shfl1 = list(range(len(unhealthy)))\n",
    "random.shuffle(shfl1)\n",
    "\n",
    "test = []\n",
    "for i in range(3):\n",
    "    test.append( (healthy[shfl0[3*i]], unhealthy[shfl1[3*i]], torch.zeros(1, 1).to('cuda')) )\n",
    "    test.append( (healthy[shfl0[3*i + 1]], healthy[shfl0[3*i + 2]], torch.ones(1, 1).to('cuda')) )\n",
    "    test.append( (unhealthy[shfl1[3*i + 1]], unhealthy[shfl1[3*i + 2]], torch.ones(1, 1).to('cuda')) )\n",
    "\n",
    "shfl0 = shfl0[9:]\n",
    "shfl1 = shfl1[9:]\n",
    "\n",
    "train = []\n",
    "\n",
    "for i in range(7):\n",
    "    train.append( (unhealthy[shfl1[2*i]], unhealthy[shfl1[2*i + 1]], torch.ones(1, 1).to('cuda')) )\n",
    "\n",
    "shfl1 = shfl1[14:]\n",
    "\n",
    "for i in range(len(shfl1)):\n",
    "    train.append( (healthy[shfl0[i]], unhealthy[shfl1[i]], torch.zeros(1, 1).to('cuda')) )\n",
    "\n",
    "shfl0 = shfl0[len(shfl1):]\n",
    "\n",
    "for i in range(0, len(shfl0), 2):\n",
    "    train.append( (healthy[shfl0[i + 1]], healthy[shfl0[i]], torch.ones(1, 1).to('cuda')) )\n",
    "\n",
    "\n",
    "len(test), len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_net = SiameseNetwork().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sepuh/workspace/diploma/venv/lib/python3.10/site-packages/torch/nn/functional.py:1956: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "../aten/src/ATen/native/cuda/Loss.cu:92: operator(): block: [0,0,0], thread: [0,0,0] Assertion `input_val >= zero && input_val <= one` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m output \u001b[39m=\u001b[39m siamese_net(input1, input2)\n\u001b[1;32m      8\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, label)\n\u001b[0;32m----> 9\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     10\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     12\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/workspace/diploma/venv/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/workspace/diploma/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(siamese_net.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i, (input1, input2, label) in enumerate(train):\n",
    "        optimizer.zero_grad()\n",
    "        output = siamese_net(input1, input2)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {i+1}, Loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_MAPPING_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m test:\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mprint\u001b[39m(siamese_net(\u001b[39m*\u001b[39;49mx[:\u001b[39m2\u001b[39;49m])\u001b[39m.\u001b[39mitem(), x[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/workspace/diploma/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m, in \u001b[0;36mSiameseNetwork.forward\u001b[0;34m(self, input1, input2)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input1, input2):\n\u001b[0;32m---> 21\u001b[0m     output1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_once(input1)\n\u001b[1;32m     22\u001b[0m     output2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_once(input2)\n\u001b[1;32m     23\u001b[0m     L1_distance \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mabs(output1 \u001b[39m-\u001b[39m output2)\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mSiameseNetwork.forward_once\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_once\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(F\u001b[39m.\u001b[39mmax_pool2d(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x), \u001b[39m2\u001b[39m))\n\u001b[1;32m     14\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(F\u001b[39m.\u001b[39mmax_pool2d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x), \u001b[39m2\u001b[39m))\n\u001b[1;32m     15\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(F\u001b[39m.\u001b[39mmax_pool2d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(x), \u001b[39m2\u001b[39m))\n",
      "File \u001b[0;32m~/workspace/diploma/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/diploma/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/workspace/diploma/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_MAPPING_ERROR"
     ]
    }
   ],
   "source": [
    "for x in test:\n",
    "    print(siamese_net(*x[:2]).item(), x[2].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33df882f4233f8cae41ef20dc2bb6a72c0235e53f66198b13114c1721b77f8bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
